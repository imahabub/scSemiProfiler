{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a9b4f-5d14-4735-9fc1-1cb31faeef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import umap\n",
    "import anndata\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import faiss\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn\n",
    "from scipy import stats\n",
    "import scanpy as sc\n",
    "from numpy import linalg as LA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine as cos\n",
    "import anndata\n",
    "from matplotlib.pyplot import figure\n",
    "from multiprocessing import Process\n",
    "\n",
    "from fast_generator_cancer import *\n",
    "from fast_functions_cancer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278147a-fe75-4c16-ac96-29cf323364f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217109d-2db3-47bc-8ed5-b5e461202469",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "pids=[]\n",
    "f = open('sids.txt','r')\n",
    "lines=f.readlines()\n",
    "for l in lines:\n",
    "    pids.append(l.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda94c2-b52e-40b8-878b-9fbd83662dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprefile = 'training_rec/init_representatives_4.txt' \n",
    "clusterfile = 'training_rec/init_cluster_labels_4.txt' \n",
    "\n",
    "f= open(reprefile,'r')\n",
    "lines=f.readlines()\n",
    "init_representatives=[]\n",
    "for l in lines:\n",
    "    init_representatives.append(int(l.strip().split()[0]))\n",
    "f.close()\n",
    "\n",
    "f= open(clusterfile,'r')\n",
    "init_cluster_labels=[]\n",
    "lines=f.readlines()\n",
    "for l in lines:\n",
    "    init_cluster_labels.append(int(l.strip().split()[0]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb86d63-289d-408f-80f1-2a2f2a34cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomlist = list(range(len(pids)))\n",
    "for r in init_representatives:\n",
    "    randomlist.remove(r)\n",
    "np.random.shuffle(randomlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c50fff-2146-4759-b776-fe59cbe96a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb3e46-2c22-4456-a18d-8e929747dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluation functions\n",
    "\n",
    "def faiss_knn(query, x, n_neighbors=1):\n",
    "    n_samples = x.shape[0]\n",
    "    n_features = x.shape[1]\n",
    "    x = np.ascontiguousarray(x)\n",
    "    \n",
    "    index = faiss.IndexFlatL2(n_features)\n",
    "    #index = faiss.IndexFlatIP(n_features)\n",
    "                  \n",
    "    index.add(x)\n",
    "    \n",
    "    if n_neighbors < 2:\n",
    "        neighbors = 2\n",
    "    else: \n",
    "        neighbors = n_neighbors\n",
    "    \n",
    "    weights, targets = index.search(query, neighbors)\n",
    "\n",
    "    #sources = np.repeat(np.arange(n_samples), neighbors)\n",
    "    #targets = targets.flatten()\n",
    "    #weights = weights.flatten()\n",
    "    weights = weights[:,:n_neighbors]\n",
    "    if -1 in targets:\n",
    "        raise InternalError(\"Not enough neighbors were found. Please consider \"\n",
    "                            \"reducing the number of neighbors.\")\n",
    "    return weights\n",
    "\n",
    "def pearson_compare(query,x):\n",
    "    return 0\n",
    "\n",
    "def cos_compare(query,x):\n",
    "    return 0\n",
    "\n",
    "\n",
    "def pca_compare(query,x):\n",
    "    qx = np.concatenate([query,x],axis=0)\n",
    "    qxpca = PCA(n_components=100)\n",
    "    dx=qxpca.fit_transform(qx)\n",
    "    \n",
    "    newq = dx[:query.shape[0],:].copy(order='C')\n",
    "    newx = dx[query.shape[0]:,:].copy(order='C')\n",
    "    score = faiss_knn(newq,newx,n_neighbors=1)\n",
    "    return score\n",
    "\n",
    "def umap_compare(query,x):\n",
    "    qx = np.concatenate([query,x],axis=0)\n",
    "    qxpca = PCA(n_components=100)\n",
    "    dpca=qxpca.fit_transform(qx)\n",
    "    umap_reduc=umap.UMAP(min_dist=0.5,spread=1.0,negative_sample_rate=5 )\n",
    "    dx = umap_reduc.fit_transform(dpca)\n",
    "    newq = dx[:query.shape[0],:].copy(order='C')\n",
    "    newx = dx[query.shape[0]:,:].copy(order='C')\n",
    "    score = faiss_knn(newq,newx,n_neighbors=1)\n",
    "    return score\n",
    "\n",
    "def knncompare(query,x,n_neighbors=1,dist='PCA'):\n",
    "    if dist == 'Euclidean':\n",
    "        score = faiss_knn(query,x,n_neighbors=n_neighbors)\n",
    "        score2 = faiss_knn(x,query,n_neighbors=n_neighbors)\n",
    "    elif dist == 'Pearson':\n",
    "        score = pearson_compare(query,x)\n",
    "        score2 = pearson_compare(x,query)\n",
    "    elif dist == 'cos':\n",
    "        score = cos_compare(query,x)\n",
    "        score2 = cos_compare(x,query)\n",
    "    elif dist == 'PCA':\n",
    "        score = pca_compare(query,x)\n",
    "        score2 = pca_compare(x,query)\n",
    "    elif dist == 'UMAP':\n",
    "        score = umap_compare(query,x)\n",
    "        score2 = umap_compare(x,query)\n",
    "    else:\n",
    "        score = 0\n",
    "        print('distance option not found')\n",
    "        \n",
    "    return (score.mean() + score2.mean())/2\n",
    "\n",
    "def normtotal(x,h=1e4):\n",
    "    ratios = h/x.sum(axis=1)\n",
    "    x=(x.T*ratios).T\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859188d-f220-478a-ba66-74a7919d5fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c8fb8-9997-4903-84f6-7c02218bd944",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulkdata = anndata.read_h5ad('bulkcount.h5ad')\n",
    "print(bulkdata.X.max())\n",
    "sc.pp.log1p(bulkdata)\n",
    "sc.tl.pca(bulkdata, svd_solver='arpack',n_comps=100)\n",
    "reduced_bulk = bulkdata.obsm['X_pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c4225-f186-498e-b207-8c51c9faf476",
   "metadata": {},
   "outputs": [],
   "source": [
    "sids=pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978aac30-f63e-4ed9-bf28-58704e6d1cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### load ground truth\n",
    "\n",
    "hvmask = np.load('hvmask.npy')\n",
    "setmask = np.load('hvset.npy')\n",
    "\n",
    "gts=[]\n",
    "for i in range(len(sids)):\n",
    "    sid = sids[i]\n",
    "    adata = anndata.read_h5ad('sample_sc/' + sid + '.h5ad')\n",
    "    gt = np.array(adata[:,hvmask].X.todense())\n",
    "    gt = np.array(gt)\n",
    "    gts.append(gt)\n",
    "    print(i,end=', ')\n",
    "\n",
    "print(adata.X.max())\n",
    "del adata\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7e944-9e32-409f-85e6-98e4e271889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T  = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0bd560-9319-4638-8111-9679eefd149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdimsemis = []\n",
    "xdimgts=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c294b5-023a-4fc4-9f0d-e6f332ef9c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## active learning functions \n",
    "\n",
    "\n",
    "\n",
    "def pick_batch(reduced_bulk=reduced_bulk,\\\n",
    "                representatives=init_representatives,\\\n",
    "                cluster_labels=init_cluster_labels,\\\n",
    "                xdimsemis=xdimsemis,\\\n",
    "                xdimgts=xdimgts,\\\n",
    "                discount_rate = 1,\\\n",
    "                semi_dis_rate = 1,\\\n",
    "                batch_size=8\\\n",
    "               ):\n",
    "    # \n",
    "    lhet = []\n",
    "    lmp = [] \n",
    "    for i in range(len(representatives)):\n",
    "        cluster_heterogeneity,in_cluster_uncertainty,uncertain_patient=compute_cluster_heterogeneity(cluster_number=i,\\\n",
    "                            reduced_bulk=reduced_bulk,\\\n",
    "                           representatives=init_representatives,\\\n",
    "                            cluster_labels=init_cluster_labels,\\\n",
    "                            xdimsemis=xdimsemis,\\\n",
    "                            xdimgts=xdimgts,\\\n",
    "                            discount_rate = 1,\\\n",
    "                            semi_dis_rate = 1\\\n",
    "                           )\n",
    "        lhet.append(cluster_heterogeneity)\n",
    "        lmp.append(uncertain_patient)\n",
    "    \n",
    "    \n",
    "    new_representatives = copy.deepcopy(representatives)\n",
    "    for i in range(batch_size):\n",
    "        mp_index = np.array(lhet).argmax()\n",
    "        mp = lmp[mp_index]\n",
    "        \n",
    "        new_representatives.append(mp)\n",
    "        lhet.pop(mp_index)\n",
    "        lmp.pop(mp_index)\n",
    "    \n",
    "    new_cluster_labels= update_membership(reduced_bulk=reduced_bulk,\\\n",
    "                      representatives=new_representatives)\n",
    "    \n",
    "    return new_representatives,new_cluster_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def uncertaintybulk(reduced_bulk=reduced_bulk,\\  # select a new batch using passive learning\n",
    "                rep=init_representatives,\\\n",
    "                cl=init_cluster_labels,\\\n",
    "                batch_size=BATCH_SIZE\\\n",
    "               ):\n",
    "    \n",
    "    new_representatives = copy.deepcopy(rep)\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        distorep =[] \n",
    "    \n",
    "        for i in range(len(reduced_bulk)):\n",
    "            if i in new_representatives:\n",
    "                distorep.append(0)\n",
    "                continue\n",
    "            crep = rep[cl[i]]\n",
    "            dist = (reduced_bulk[new_representatives] - reduced_bulk[i])**2\n",
    "            distorep.append(dist.sum())\n",
    "        \n",
    "        new_representatives.append(np.array(distorep).argmax())\n",
    "    \n",
    "    \n",
    "    new_cluster_labels= update_membership(reduced_bulk=reduced_bulk,\\\n",
    "                      representatives=new_representatives)\n",
    "    \n",
    "    \n",
    "    return new_representatives,new_cluster_labels\n",
    "\n",
    "def pick_batch_eee(reduced_bulk=reduced_bulk,\\    # select a new batch using active learning\n",
    "                representatives=init_representatives,\\\n",
    "                cluster_labels=init_cluster_labels,\\\n",
    "                xdimsemis=xdimsemis,\\\n",
    "                xdimgts=xdimgts,\\\n",
    "                discount_rate = 1,\\\n",
    "                semi_dis_rate = 1,\\\n",
    "                batch_size=8\\\n",
    "               ):\n",
    "    # \n",
    "    lhet = []\n",
    "    lmp = [] \n",
    "    for i in range(len(representatives)):\n",
    "        cluster_heterogeneity,in_cluster_uncertainty,uncertain_patient=compute_cluster_heterogeneity(cluster_number=i,\\\n",
    "                            reduced_bulk=reduced_bulk,\\\n",
    "                           representatives=representatives,\\\n",
    "                            cluster_labels=cluster_labels,\\\n",
    "                            xdimsemis=xdimsemis,\\\n",
    "                            xdimgts=xdimgts,\\\n",
    "                            discount_rate = 1,\\\n",
    "                            semi_dis_rate = 1\\\n",
    "                           )\n",
    "        lhet.append(cluster_heterogeneity)\n",
    "        lmp.append(uncertain_patient)\n",
    "    \n",
    "    new_representatives = copy.deepcopy(representatives)\n",
    "    new_cluster_labels = copy.deepcopy(cluster_labels)\n",
    "    print('heterogeneities: ',lhet)\n",
    "    for i in range(batch_size):\n",
    "        new_num = len(new_representatives)\n",
    "        mp_index = np.array(lhet).argmax()\n",
    "        print(mp_index)\n",
    "        lhet[mp_index] = -999\n",
    "        bestp, new_cluster_labels, hets = best_patient(cluster_labels=new_cluster_labels,representatives=new_representatives,\\\n",
    "                 reduced_bulk=reduced_bulk,cluster_num=mp_index,new_num=new_num)\n",
    "        \n",
    "        new_representatives = new_representatives + [bestp]\n",
    "    \n",
    "    return new_representatives,new_cluster_labels\n",
    "\n",
    "def best_patient(cluster_labels=init_cluster_labels,representatives=init_representatives,\\\n",
    "                 reduced_bulk=reduced_bulk,cluster_num=0,new_num=None):\n",
    "    if new_num == None:\n",
    "        new_num = len(representatives)\n",
    "    pindices = np.where(np.array(cluster_labels)==cluster_num)[0]\n",
    "    representative = representatives[cluster_num]\n",
    "    hets=[]\n",
    "    potential_new_labels = []\n",
    "    for i in range(len(pindices)):\n",
    "        potential_new_label = copy.deepcopy(cluster_labels)\n",
    "        newrepre = pindices[i]\n",
    "        het = 0\n",
    "        if newrepre in representatives:\n",
    "            hets.append(9999)\n",
    "            potential_new_labels.append(potential_new_label)\n",
    "            continue\n",
    "        for j in range(len(pindices)):\n",
    "            brepre = reduced_bulk[representative]\n",
    "            brepre2 = reduced_bulk[newrepre]\n",
    "            bj = reduced_bulk[pindices[j]]\n",
    "            bdist1 = (brepre - bj)**2\n",
    "            bdist1 = bdist1.sum()\n",
    "            bdist1 = bdist1**0.5\n",
    "            bdist2 = (brepre2 - bj)**2\n",
    "            bdist2 = bdist2.sum()\n",
    "            bdist2 = bdist2**0.5\n",
    "            \n",
    "            if bdist1 > bdist2:\n",
    "                #print(pindices[j])\n",
    "                het = het + bdist2\n",
    "                potential_new_label[pindices[j]]=new_num\n",
    "            else:\n",
    "                het = het + bdist1\n",
    "        hets.append(het)\n",
    "        potential_new_labels.append(potential_new_label)\n",
    "    hets = np.array(hets)\n",
    "    bestp = pindices[np.argmin(hets)]\n",
    "    new_cluster_labels = potential_new_labels[np.argmin(hets)]\n",
    "    return bestp, new_cluster_labels, hets\n",
    "\n",
    "def update_membership(reduced_bulk=reduced_bulk,\\\n",
    "                      representatives=init_representatives,\\\n",
    "                      \n",
    "                     ):\n",
    "    new_cluster_labels = []\n",
    "    for i in range(len(reduced_bulk)):\n",
    "        \n",
    "        dists=[]\n",
    "        #dist to repres\n",
    "        for j in representatives:\n",
    "            bdist = (reduced_bulk[j] - reduced_bulk[i])**2 \n",
    "            bdist = bdist.sum()\n",
    "            bdist = bdist**0.5\n",
    "            dists.append(bdist)\n",
    "        membership = np.array(dists).argmin()\n",
    "        new_cluster_labels.append(membership)\n",
    "    return new_cluster_labels\n",
    "\n",
    "def compute_cluster_heterogeneity(cluster_number=0,\\\n",
    "                            reduced_bulk=reduced_bulk,\\\n",
    "                           representatives=init_representatives,\\\n",
    "                            cluster_labels=init_cluster_labels,\\\n",
    "                            xdimsemis=xdimsemis,\\\n",
    "                            xdimgts=xdimgts,\\\n",
    "                            discount_rate = 1,\\\n",
    "                            semi_dis_rate = 1\\\n",
    "                           ):\n",
    "    semiflag=0\n",
    "    \n",
    "    representative = representatives[cluster_number]\n",
    "    in_cluster_uncertainty = []\n",
    "    cluster_labels = np.array(cluster_labels)\n",
    "    cluster_patient_indices = np.where(cluster_labels==cluster_number)[0]\n",
    "    \n",
    "    for i in range(len(cluster_patient_indices)): # number of patients in this cluster except the representative\n",
    "        \n",
    "        patient_index = cluster_patient_indices[i]\n",
    "        \n",
    "        if patient_index in representatives:\n",
    "            in_cluster_uncertainty.append(0)\n",
    "            continue\n",
    "            \n",
    "        # distance between this patient and representative\n",
    "        bdist = (reduced_bulk[representative] - reduced_bulk[patient_index])**2 \n",
    "        bdist = bdist.sum()\n",
    "        bdist = bdist**0.5\n",
    "        \n",
    "        ma = np.array(xdimsemis[patient_index]).copy(order='C')\n",
    "        mb = np.array(xdimgts[representative]).copy(order='C')\n",
    "        sdist = (faiss_knn(ma,mb,n_neighbors=1).mean())\n",
    "        \n",
    "        semiloss = np.log(1+gts[patient_index].sum(axis=0))- np.log(1+semis[patient_index].sum(axis=0))\n",
    "        semiloss = semiloss**2\n",
    "        semiloss = semiloss.sum()\n",
    "        semiloss = semiloss**0.5\n",
    "        \n",
    "        #print(bdist,sdist,semiloss)\n",
    "        \n",
    "        uncertainty = bdist + sdist*discount_rate + semi_dis_rate * semiloss\n",
    "        \n",
    "        in_cluster_uncertainty.append(uncertainty)\n",
    "        \n",
    "    cluster_heterogeneity = np.array(in_cluster_uncertainty).sum()\n",
    "    \n",
    "    uncertain_patient = cluster_patient_indices[np.array(in_cluster_uncertainty).argmax()] \n",
    "\n",
    "    return cluster_heterogeneity,in_cluster_uncertainty,uncertain_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92942555-7528-40bb-9f31-8ada0b7b63ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3dcd16fa-1740-4993-82d0-28d294807036",
   "metadata": {},
   "source": [
    "X = np.concatenate(gts,axis=0)\n",
    "t_start = timeit.default_timer()\n",
    "X = np.log(X+1)\n",
    "reducer =  PCA(n_components = 100)\n",
    "X_reduced = reducer.fit_transform(X)\n",
    "t_end = timeit.default_timer()\n",
    "print()\n",
    "print(str(t_end-t_start),'for pca')\n",
    "\n",
    "\n",
    "pcagts = []\n",
    "offset = 0\n",
    "for i in range(len(pids)):\n",
    "    lendata = gts[i].shape[0]\n",
    "    pcagts.append(X_reduced[offset:offset+lendata])\n",
    "t_end = timeit.default_timer()\n",
    "\n",
    "\n",
    "t_start = timeit.default_timer()\n",
    "\n",
    "maxdist = 0\n",
    "maxpair = None\n",
    "for i in range(len(pids)):\n",
    "    for j in range(len(pids)):\n",
    "        ma = np.array(pcagts[i]).copy(order='C')\n",
    "        mb = np.array(pcagts[j]).copy(order='C')\n",
    "        ubscore = list(faiss_knn(ma,mb,n_neighbors=1)) + list(faiss_knn(mb,ma,n_neighbors=1))\n",
    "        ubscore = np.array(ubscore)\n",
    "        ubscore = ubscore.mean()\n",
    "        if ubscore > maxdist:\n",
    "            maxdist = ubscore\n",
    "            maxpair = (i,j)\n",
    "t_start = timeit.default_timer()\n",
    "        \n",
    "print()\n",
    "print(str(t_end-t_start),'for finding furthest pair')\n",
    "print(maxpair)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c1a6e-d686-4c4c-8e20-de495b634b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=42\n",
    "B=91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ada6b2-ca8b-4cd8-8003-0f4a77853d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9fd66-5ffd-42e5-b332-64ed5a1a4784",
   "metadata": {},
   "source": [
    "### start semiprofiling loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de708c-90ae-473b-91f5-71571887d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi processes\n",
    "\n",
    "def multisemi(l,device,rnd):\n",
    "    if l ==[]:\n",
    "        return\n",
    "    for i in l:\n",
    "        print('start semiprofiling patient',str(i),'using',device)\n",
    "        os.system('/mnt/data/jingtao2/anaconda3/envs/deep/bin/python semicommand.py '+str(i)+' '+str(device) + ' '+str(rnd))\n",
    "    return \n",
    "\n",
    "def multirecon(i,device):\n",
    "    print('start recon patient',str(i),'using',device)\n",
    "    pid=pids[i]\n",
    "    fastrecon(pid=pid,tgtpid=None,device=device,k=15,diagw=1,vaesteps=100,gansteps=100,save=True,path=None)\n",
    "    return\n",
    "\n",
    "def multirecon2(i,reconmodel,device):\n",
    "    print('start recon2 patient',str(i),'using',device)\n",
    "    pid=pids[i]\n",
    "    reconst_pretrain2(pid,reconmodel,device,k=15,diagw=1.0,vaesteps=50,gansteps=50,save=True)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa3742-9554-42dd-918f-d33de9e7fc2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    for seqp in range(4,113,BATCH_SIZE):\n",
    "        ### load this rounds representatives and cluster labels\n",
    "        if seqp <= BATCH_SIZE:\n",
    "            rnd=1\n",
    "            print('start initial round')\n",
    "            reprefile = 'compare_training_rec/init_representatives_'+str(BATCH_SIZE)+'.txt' \n",
    "            clusterfile = 'compare_training_rec/init_cluster_labels_'+str(BATCH_SIZE)+'.txt' \n",
    "\n",
    "            f = open(reprefile,'r')\n",
    "            lines=f.readlines()\n",
    "            representatives=[]\n",
    "            for l in lines:\n",
    "                representatives.append(int(l.strip().split()[0]))\n",
    "            f.close()\n",
    "\n",
    "            f= open(clusterfile,'r')\n",
    "            cluster_labels=[]\n",
    "            lines=f.readlines()\n",
    "            for l in lines:\n",
    "                cluster_labels.append(int(l.strip().split()[0]))\n",
    "            f.close()\n",
    "            \n",
    "            naive_representatives = representatives\n",
    "            naive_cluster_labels = cluster_labels\n",
    "            \n",
    "        else:\n",
    "            rnd = int(seqp/4)\n",
    "            print('start round '+str(rnd))\n",
    "            reprefile = 'compare_training_rec/eer_representatives_'+str(rnd)+'.txt' \n",
    "            clusterfile = 'compare_training_rec/eer_cluster_labels_'+str(rnd)+'.txt' \n",
    "\n",
    "            f= open(reprefile,'r')\n",
    "            lines=f.readlines()\n",
    "            representatives=[]\n",
    "            for l in lines:\n",
    "                representatives.append(int(l.strip().split()[0]))\n",
    "            f.close()\n",
    "\n",
    "            f= open(clusterfile,'r')\n",
    "            cluster_labels=[]\n",
    "            lines=f.readlines()\n",
    "            for l in lines:\n",
    "                cluster_labels.append(int(l.strip().split()[0]))\n",
    "            f.close()\n",
    "        ### end of loading representatives and cluster labels\n",
    "\n",
    "\n",
    "\n",
    "        ### reconstruction stage 1\n",
    "        c=0\n",
    "        #procs=[]\n",
    "        for rp in representatives:#new_representatives:\n",
    "            pid = pids[rp]\n",
    "            if ('fast_reconst1_'+pid) in os.listdir('cancer_models'):\n",
    "                print('recon' + str(pid) + 'exist')\n",
    "                continue\n",
    "            device = 'cuda:' + str(c%8)\n",
    "            fastrecon(pid=pid,tgtpid=None,device=device,k=15,diagw=1,vaesteps=100,gansteps=100,save=True,path=None)\n",
    "            \n",
    "            #proc = Process(target=multirecon, args=(rp,device))\n",
    "            #procs.append(proc)\n",
    "            #proc.start()\n",
    "            c=c+1\n",
    "        \n",
    "        #for proc in procs:\n",
    "        #        proc.join()\n",
    "        ### end of recon stage 1\n",
    "\n",
    "        ### reconstruction stage 2\n",
    "        i=0\n",
    "        c=0\n",
    "        procs=[]\n",
    "        for rp in representatives:##new_representatives:\n",
    "            pid = pids[rp]\n",
    "            device = 'cuda:' + str(c%8)\n",
    "            if ('fastreconst2_'+pid) in os.listdir('cancer_models'):\n",
    "                print('recon2' + str(pid) + 'exist')\n",
    "                continue\n",
    "            adata,adj,variances,bulk,geneset_len = setdata(pid,None,device=device,k=15,diagw=1.0)\n",
    "            reconmodel = fastgenerator(adj = adj,variances = variances,markermask = None,bulk=bulk,geneset_len = geneset_len,adata=adata,\\\n",
    "                        n_hidden=256,n_latent=32,dropout_rate=0,countbulkweight=0,logbulkweight=0,absbulkweight=0,abslogbulkweight=0,\\\n",
    "                        power=2,corrbulkweight=0,meanbias=0)\n",
    "            reconmodel.module.load_state_dict(torch.load('cancer_models/fast_reconst1_'+str(pid)))\n",
    "            reconst_pretrain2(pid,reconmodel,device,k=15,diagw=1.0,vaesteps=50,gansteps=50,save=True)\n",
    "            \n",
    "            \n",
    "            #proc = Process(target=multirecon2, args=(rp,reconmodel,device))\n",
    "            #procs.append(proc)\n",
    "            #proc.start()\n",
    "            #c=c+1\n",
    "            #i+=1\n",
    "        #for proc in procs:\n",
    "        #        proc.join()\n",
    "        ### end of recon stage 2\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        ### semi profiling\n",
    "        procs = []\n",
    "        c=0\n",
    "\n",
    "        l0 = []\n",
    "        l1 = []\n",
    "        l2 = []\n",
    "        l3 = []\n",
    "        l4 = []\n",
    "        l5 = []\n",
    "        l6 = []\n",
    "        l7 = []\n",
    "\n",
    "        for i in range(0,14):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l0.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(14,28):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l1.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(28,42):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l2.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(42,56):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l3.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "                \n",
    "        \n",
    "        for i in range(56,70):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l4.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(70,84):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l5.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(84,98):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l6.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(98,112):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l7.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        proc = Process(target=multisemi, args=(l0,'cuda:0',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l1,'cuda:1',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l2,'cuda:2',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l3,'cuda:3',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        \n",
    "        \n",
    "        proc = Process(target=multisemi, args=(l4,'cuda:4',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l5,'cuda:5',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l6,'cuda:6',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l7,'cuda:7',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "        ### end semi \n",
    "\n",
    "\n",
    "        ### load new semi results\n",
    "        t_start = timeit.default_timer()\n",
    "        semis=[]\n",
    "        for i in range(len(pids)):\n",
    "            pid = pids[i]\n",
    "            repre = representatives[cluster_labels[i]]\n",
    "            reprepid = pids[repre]\n",
    "            if reprepid==pid:\n",
    "                semis.append(gts[i])\n",
    "            else:\n",
    "                xsem = np.load('semidata/fast'+ reprepid+'_to_'+pid+'.npy')\n",
    "                xsem = np.array(xsem)\n",
    "                xsem = xsem*(xsem>T)                                   ############### thresholding\n",
    "                semis.append(xsem)\n",
    "            print(i,end=', ')\n",
    "\n",
    "        t_end = timeit.default_timer()\n",
    "        print()\n",
    "        print(str(t_end-t_start),'for loading semi')\n",
    "        ### end of loading semi\n",
    "\n",
    "\n",
    "\n",
    "        ### PCA\n",
    "        t_start = timeit.default_timer()\n",
    "        \n",
    "        X = np.concatenate([np.concatenate(gts,axis=0),np.concatenate(semis,axis=0)],axis=0)\n",
    "        X = np.log(X+1)\n",
    "        reducer =  PCA(n_components = 100)#,svd_solver = 'randomized')#randomized_svd(n_components=100)  #PCA(n_components=100)#\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "\n",
    "        t_end = timeit.default_timer()\n",
    "        print()\n",
    "        print(str(t_end-t_start),'for pca')\n",
    "        ### end of pca\n",
    "\n",
    "\n",
    "        ### reduced data for patients\n",
    "        xdimgts=[]\n",
    "        xdimsemis=[]\n",
    "        offset=0\n",
    "        xused = X_reduced#X_UMAP # X_PCA\n",
    "        for i in range(len(pids)):\n",
    "            xdimgts.append(xused[offset:(offset+gts[i].shape[0]),:])\n",
    "            offset = offset+gts[i].shape[0]\n",
    "        lengt = offset\n",
    "        for i in range(len(pids)):\n",
    "            xdimsemis.append(xused[offset:(offset+semis[i].shape[0]),:])\n",
    "            offset = offset+semis[i].shape[0]\n",
    "        ### end \n",
    "\n",
    "        \n",
    "        ### lowerbound\n",
    "        lbscores2 = []\n",
    "        lbgt = copy.deepcopy(X_reduced)#[:lengt])\n",
    "        np.random.shuffle(lbgt)\n",
    "        lbgt1 = lbgt[:lbgt.shape[0]//2,:]\n",
    "        lbgt2 = lbgt[lbgt.shape[0]//2:,:]\n",
    "        ma = np.array(lbgt1).copy(order='C')\n",
    "        mb = np.array(lbgt2).copy(order='C')\n",
    "        lowerbound = list(faiss_knn(mb,ma,n_neighbors=1)) + list(faiss_knn(ma,mb,n_neighbors=1))\n",
    "        lowerbound = (np.array(lowerbound)).mean()\n",
    "\n",
    "        ### upperbound\n",
    "        \n",
    "        ubscores = []\n",
    "        for i in range(len(pids)):\n",
    "            gt = xdimgts[i]\n",
    "            randomidx = np.random.randint(0,len(pids))\n",
    "            gtr = xdimgts[randomidx]\n",
    "            ma = np.array(gt).copy(order='C')\n",
    "            mb = np.array(gtr).copy(order='C')\n",
    "            ubscore = list(faiss_knn(ma,mb,n_neighbors=1)) + list(faiss_knn(mb,ma,n_neighbors=1))\n",
    "            ubscore = np.array(ubscore)\n",
    "            ubscores.append(np.array(ubscore).mean())\n",
    "        upperbound = np.array(ubscores).mean()\n",
    "        \n",
    "        '''\n",
    "        ma = np.array(xdimgts[A]).copy(order='C')\n",
    "        mb = np.array(xdimgts[B]).copy(order='C')\n",
    "        ubscore = list(faiss_knn(ma,mb,n_neighbors=1)) + list(faiss_knn(mb,ma,n_neighbors=1))\n",
    "        ubscore = np.array(ubscore)\n",
    "        upperbound = ubscore.mean()'''\n",
    "        \n",
    "        ### semi evaluation\n",
    "        t_start = timeit.default_timer()\n",
    "        scores = []\n",
    "        for i in range(len(pids)):\n",
    "            pid = pids[i]\n",
    "            if i in representatives:\n",
    "                scores.append(lowerbound)\n",
    "                continue\n",
    "            gt = xdimgts[i]\n",
    "            xs = xdimsemis[i]\n",
    "            ma = np.array(gt).copy(order='C')\n",
    "            mb = np.array(xs).copy(order='C')\n",
    "            err1 = faiss_knn(ma,mb,n_neighbors=1)\n",
    "            err2 = faiss_knn(mb,ma,n_neighbors=1)\n",
    "            err = list(err1) + list(err2)\n",
    "            err = (np.array(err)).mean()\n",
    "            scores.append(err)\n",
    "        semierror = np.array(scores).mean()\n",
    "\n",
    "        \n",
    "        ### new naive by kmeans\n",
    "        '''\n",
    "        naivescores = []\n",
    "        kmeans = KMeans(n_clusters=len(representatives), random_state=0).fit(bulkdata.obsm['X_pca'])\n",
    "        naive_cluster_labels = kmeans.labels_\n",
    "        pnums = []\n",
    "        for i in range(len(bulkdata.X)):\n",
    "            pnums.append(i)\n",
    "        pnums=np.array(pnums)\n",
    "        centers=[]\n",
    "        naive_representatives=[]\n",
    "        for i in range(len(np.unique(naive_cluster_labels))):\n",
    "            mask = (naive_cluster_labels==i)\n",
    "            cluster = bulkdata.obsm['X_pca'][mask]\n",
    "            cluster_patients = pnums[mask]\n",
    "            center = cluster.mean(axis=0)\n",
    "            centers.append(center)\n",
    "            # find the closest patient\n",
    "            sqdist = ((cluster - center)**2).sum(axis=1)\n",
    "            cluster_representative = cluster_patients[np.argmin(sqdist)]\n",
    "            naive_representatives.append(cluster_representative)\n",
    "\n",
    "        \n",
    "        for i in range(len(pids)):\n",
    "            pid = pids[i]\n",
    "            if i in naive_representatives:\n",
    "                naivescores.append(lowerbound)\n",
    "                continue\n",
    "            gt = xdimgts[i]\n",
    "            repre = naive_representatives[naive_cluster_labels[i]]\n",
    "            xs = xdimgts[repre]\n",
    "            ma = np.array(gt).copy(order='C')\n",
    "            mb = np.array(xs).copy(order='C')\n",
    "            err1 = faiss_knn(ma,mb,n_neighbors=1)\n",
    "            err2 = faiss_knn(mb,ma,n_neighbors=1)\n",
    "            err = list(err1) + list(err2)\n",
    "            err = (np.array(err)).mean()\n",
    "            naivescores.append(err)\n",
    "        naiveerror = np.array(naivescores).mean()\n",
    "        '''\n",
    "        \n",
    "        ### naive evaluation\n",
    "        '''\n",
    "        naivescores = []\n",
    "        for i in range(len(pids)):\n",
    "            pid = pids[i]\n",
    "            if i in representatives:\n",
    "                naivescores.append(lowerbound)\n",
    "                continue\n",
    "            gt = xdimgts[i]\n",
    "            repre = representatives[cluster_labels[i]]\n",
    "            xs = xdimgts[repre]\n",
    "            ma = np.array(gt).copy(order='C')\n",
    "            mb = np.array(xs).copy(order='C')\n",
    "            err1 = faiss_knn(ma,mb,n_neighbors=1)\n",
    "            err2 = faiss_knn(mb,ma,n_neighbors=1)\n",
    "            err = list(err1) + list(err2)\n",
    "            err = (np.array(err)).mean()\n",
    "            naivescores.append(err)\n",
    "        naiveerror = np.array(naivescores).mean()'''\n",
    "        \n",
    "        \n",
    "        ### new naive\n",
    "        naivescores = []\n",
    "        for i in range(len(pids)):\n",
    "            pid = pids[i]\n",
    "            if i in naive_representatives:\n",
    "                naivescores.append(lowerbound)\n",
    "                continue\n",
    "            gt = xdimgts[i]\n",
    "            repre = naive_representatives[naive_cluster_labels[i]]\n",
    "            xs = xdimgts[repre]\n",
    "            ma = np.array(gt).copy(order='C')\n",
    "            mb = np.array(xs).copy(order='C')\n",
    "            err1 = faiss_knn(ma,mb,n_neighbors=1)\n",
    "            err2 = faiss_knn(mb,ma,n_neighbors=1)\n",
    "            err = list(err1) + list(err2)\n",
    "            err = (np.array(err)).mean()\n",
    "            naivescores.append(err)\n",
    "        naiveerror = np.array(naivescores).mean()\n",
    "        \n",
    "        naive_representatives,naive_cluster_labels = naivebatch(\n",
    "                reduced_bulk=reduced_bulk,\\\n",
    "                representatives = naive_representatives,\\\n",
    "                cluster_labels = naive_cluster_labels,\\\n",
    "                batch_size = 4)\n",
    "\n",
    "        normednaive = (naiveerror - lowerbound)/(upperbound - lowerbound)\n",
    "        normedsemi = (semierror - lowerbound)/(upperbound - lowerbound)\n",
    "        t_end = timeit.default_timer()\n",
    "        print('naive:',naiveerror)\n",
    "        print('semi:',semierror)\n",
    "        print('upperbound:',upperbound)\n",
    "        print('lowerbound;',lowerbound)\n",
    "        print('normed naive:',normednaive)\n",
    "        print('normed semi:',normedsemi)\n",
    "        print(str(t_end-t_start),' for evaluation')\n",
    "        \n",
    "        f=open('performance/finalactive_cancer_'+str(rnd)+'.txt','w')\n",
    "        f.write('naive:'+str(naiveerror)+'\\n')\n",
    "        f.write('semi:'+str(semierror)+'\\n')\n",
    "        f.write('upperbound:'+str(upperbound)+'\\n')\n",
    "        f.write('lowerbound;'+str(lowerbound)+'\\n')\n",
    "        f.write('normed naive:'+str(normednaive)+'\\n')\n",
    "        f.write('normed semi:'+str(normedsemi)+'\\n')\n",
    "        f.close()\n",
    "        ### end of evaluation\n",
    "\n",
    "\n",
    "        ### active learning \n",
    "        \n",
    "        nrep, nlabels = pick_batch_eee(reduced_bulk=reduced_bulk,\\\n",
    "                        representatives=representatives,\\\n",
    "                        cluster_labels=cluster_labels,\\\n",
    "                        xdimsemis=xdimsemis,\\\n",
    "                        xdimgts=xdimgts,\\\n",
    "                        discount_rate = 1,\\\n",
    "                        semi_dis_rate = 1,\\\n",
    "                        batch_size=4\\\n",
    "                       )\n",
    "\n",
    "        new_representatives = nrep\n",
    "        new_cluster_labels = nlabels\n",
    "        if ('eer_cluster_labels' + str(rnd+1) + \".txt\") in os.listdir('training_rec'):\n",
    "            cluster_labels = new_cluster_labels\n",
    "            representatives = new_representatives\n",
    "            continue\n",
    "            \n",
    "        f=open('training_rec/eer_cluster_labels_'+str(rnd+1)+'.txt','w')\n",
    "        for i in range(len(new_cluster_labels)):\n",
    "            f.write(str(new_cluster_labels[i])+'\\n')\n",
    "        f.close()\n",
    "        f=open('training_rec/eer_representatives_'+str(rnd+1)+'.txt','w')\n",
    "        for i in range(len(new_representatives)):\n",
    "            f.write(str(new_representatives[i])+'\\n')\n",
    "        f.close()\n",
    "\n",
    "        cluster_labels = new_cluster_labels\n",
    "        representatives = new_representatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7d528-043e-468e-b506-acb258028d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8970ea3-ad61-45a3-a933-a0d2851f1be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835ac6d-fbc3-4b1a-b136-3ea973f489ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
