{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a9b4f-5d14-4735-9fc1-1cb31faeef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import umap\n",
    "import anndata\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import faiss\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn\n",
    "from scipy import stats\n",
    "import scanpy as sc\n",
    "from numpy import linalg as LA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine as cos\n",
    "import anndata\n",
    "from matplotlib.pyplot import figure\n",
    "from multiprocessing import Process\n",
    "\n",
    "from fast_generator_covid import *\n",
    "from fast_functions_covid import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217109d-2db3-47bc-8ed5-b5e461202469",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "pids=[]\n",
    "f = open('sids.txt','r')\n",
    "lines=f.readlines()\n",
    "for l in lines:\n",
    "    pids.append(l.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda94c2-b52e-40b8-878b-9fbd83662dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprefile = 'training_rec/init_representatives_4.txt' \n",
    "clusterfile = 'training_rec/init_cluster_labels_4.txt' \n",
    "\n",
    "\n",
    "\n",
    "f= open(reprefile,'r')\n",
    "lines=f.readlines()\n",
    "init_representatives=[]\n",
    "for l in lines:\n",
    "    init_representatives.append(int(l.strip().split()[0]))\n",
    "f.close()\n",
    "\n",
    "f= open(clusterfile,'r')\n",
    "init_cluster_labels=[]\n",
    "lines=f.readlines()\n",
    "for l in lines:\n",
    "    init_cluster_labels.append(int(l.strip().split()[0]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb86d63-289d-408f-80f1-2a2f2a34cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb3e46-2c22-4456-a18d-8e929747dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluation functions\n",
    "\n",
    "def faiss_knn(query, x, n_neighbors=1):\n",
    "    n_samples = x.shape[0]\n",
    "    n_features = x.shape[1]\n",
    "    x = np.ascontiguousarray(x)\n",
    "    \n",
    "    index = faiss.IndexFlatL2(n_features)\n",
    "    #index = faiss.IndexFlatIP(n_features)\n",
    "                  \n",
    "    index.add(x)\n",
    "    \n",
    "    if n_neighbors < 2:\n",
    "        neighbors = 2\n",
    "    else: \n",
    "        neighbors = n_neighbors\n",
    "    \n",
    "    weights, targets = index.search(query, neighbors)\n",
    "\n",
    "    #sources = np.repeat(np.arange(n_samples), neighbors)\n",
    "    #targets = targets.flatten()\n",
    "    #weights = weights.flatten()\n",
    "    weights = weights[:,:n_neighbors]\n",
    "    if -1 in targets:\n",
    "        raise InternalError(\"Not enough neighbors were found. Please consider \"\n",
    "                            \"reducing the number of neighbors.\")\n",
    "    return weights\n",
    "\n",
    "def pearson_compare(query,x):\n",
    "    return 0\n",
    "\n",
    "def cos_compare(query,x):\n",
    "    return 0\n",
    "\n",
    "\n",
    "def pca_compare(query,x):\n",
    "    qx = np.concatenate([query,x],axis=0)\n",
    "    qxpca = PCA(n_components=100)\n",
    "    dx=qxpca.fit_transform(qx)\n",
    "    \n",
    "    newq = dx[:query.shape[0],:].copy(order='C')\n",
    "    newx = dx[query.shape[0]:,:].copy(order='C')\n",
    "    score = faiss_knn(newq,newx,n_neighbors=1)\n",
    "    return score\n",
    "\n",
    "def umap_compare(query,x):\n",
    "    qx = np.concatenate([query,x],axis=0)\n",
    "    qxpca = PCA(n_components=100)\n",
    "    dpca=qxpca.fit_transform(qx)\n",
    "    umap_reduc=umap.UMAP(min_dist=0.5,spread=1.0,negative_sample_rate=5 )\n",
    "    dx = umap_reduc.fit_transform(dpca)\n",
    "    newq = dx[:query.shape[0],:].copy(order='C')\n",
    "    newx = dx[query.shape[0]:,:].copy(order='C')\n",
    "    score = faiss_knn(newq,newx,n_neighbors=1)\n",
    "    return score\n",
    "\n",
    "def knncompare(query,x,n_neighbors=1,dist='PCA'):\n",
    "    if dist == 'Euclidean':\n",
    "        score = faiss_knn(query,x,n_neighbors=n_neighbors)\n",
    "        score2 = faiss_knn(x,query,n_neighbors=n_neighbors)\n",
    "    elif dist == 'Pearson':\n",
    "        score = pearson_compare(query,x)\n",
    "        score2 = pearson_compare(x,query)\n",
    "    elif dist == 'cos':\n",
    "        score = cos_compare(query,x)\n",
    "        score2 = cos_compare(x,query)\n",
    "    elif dist == 'PCA':\n",
    "        score = pca_compare(query,x)\n",
    "        score2 = pca_compare(x,query)\n",
    "    elif dist == 'UMAP':\n",
    "        score = umap_compare(query,x)\n",
    "        score2 = umap_compare(x,query)\n",
    "    else:\n",
    "        score = 0\n",
    "        print('distance option not found')\n",
    "        \n",
    "    return (score.mean() + score2.mean())/2\n",
    "\n",
    "def normtotal(x,h=1e4):\n",
    "    ratios = h/x.sum(axis=1)\n",
    "    x=(x.T*ratios).T\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56946e79-432f-4efd-a9c9-3fc4bef112b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c8fb8-9997-4903-84f6-7c02218bd944",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulkdata = anndata.read_h5ad('bulkdata.h5ad')\n",
    "print(bulkdata.X.max())\n",
    "sc.pp.log1p(bulkdata)\n",
    "sc.tl.pca(bulkdata, svd_solver='arpack',n_comps=100)\n",
    "reduced_bulk = bulkdata.obsm['X_pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c4225-f186-498e-b207-8c51c9faf476",
   "metadata": {},
   "outputs": [],
   "source": [
    "sids=pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978aac30-f63e-4ed9-bf28-58704e6d1cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### load ground truth\n",
    "\n",
    "hvmask = np.load('hvmask.npy')\n",
    "setmask = np.load('hvset.npy')\n",
    "\n",
    "gts=[]\n",
    "for i in range(len(sids)):\n",
    "    sid = sids[i]\n",
    "    adata = anndata.read_h5ad('sample_sc/' + sid + '.h5ad')\n",
    "    gt = np.array(adata[:,hvmask].X.todense())\n",
    "    gts.append(gt)\n",
    "    print(i,end=', ')\n",
    "\n",
    "print(adata.X.max())\n",
    "del adata\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0bd560-9319-4638-8111-9679eefd149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdimsemis = []\n",
    "xdimgts=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c294b5-023a-4fc4-9f0d-e6f332ef9c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## active learning functions \n",
    "def pick_batch(reduced_bulk=reduced_bulk,\\\n",
    "                representatives=init_representatives,\\\n",
    "                cluster_labels=init_cluster_labels,\\\n",
    "                xdimsemis=xdimsemis,\\\n",
    "                xdimgts=xdimgts,\\\n",
    "                discount_rate = 1,\\\n",
    "                semi_dis_rate = 1,\\\n",
    "                batch_size=8\\\n",
    "               ):\n",
    "    # \n",
    "    lhet = []\n",
    "    lmp = [] \n",
    "    for i in range(len(representatives)):\n",
    "        cluster_heterogeneity,in_cluster_uncertainty,uncertain_patient=compute_cluster_heterogeneity(cluster_number=i,\\\n",
    "                            reduced_bulk=reduced_bulk,\\\n",
    "                           representatives=init_representatives,\\\n",
    "                            cluster_labels=init_cluster_labels,\\\n",
    "                            xdimsemis=xdimsemis,\\\n",
    "                            xdimgts=xdimgts,\\\n",
    "                            discount_rate = 1,\\\n",
    "                            semi_dis_rate = 1\\\n",
    "                           )\n",
    "        lhet.append(cluster_heterogeneity)\n",
    "        lmp.append(uncertain_patient)\n",
    "    \n",
    "    \n",
    "    new_representatives = copy.deepcopy(representatives)\n",
    "    for i in range(batch_size):\n",
    "        mp_index = np.array(lhet).argmax()\n",
    "        mp = lmp[mp_index]\n",
    "        \n",
    "        new_representatives.append(mp)\n",
    "        lhet.pop(mp_index)\n",
    "        lmp.pop(mp_index)\n",
    "    \n",
    "    new_cluster_labels= update_membership(reduced_bulk=reduced_bulk,\\\n",
    "                      representatives=new_representatives)\n",
    "    \n",
    "    return new_representatives,new_cluster_labels\n",
    "\n",
    "\n",
    "def uncertaintybulk(reduced_bulk=reduced_bulk,\\  # select a new batch using passive learning\n",
    "                rep=init_representatives,\\\n",
    "                cl=init_cluster_labels,\\\n",
    "                batch_size=BATCH_SIZE\\\n",
    "               ):\n",
    "    \n",
    "    new_representatives = copy.deepcopy(rep)\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        distorep =[] \n",
    "    \n",
    "        for i in range(len(reduced_bulk)):\n",
    "            if i in new_representatives:\n",
    "                distorep.append(0)\n",
    "                continue\n",
    "            crep = rep[cl[i]]\n",
    "            dist = (reduced_bulk[new_representatives] - reduced_bulk[i])**2\n",
    "            distorep.append(dist.sum())\n",
    "        \n",
    "        new_representatives.append(np.array(distorep).argmax())\n",
    "    \n",
    "    \n",
    "    new_cluster_labels= update_membership(reduced_bulk=reduced_bulk,\\\n",
    "                      representatives=new_representatives)\n",
    "    \n",
    "    \n",
    "    return new_representatives,new_cluster_labels\n",
    "\n",
    "def pick_batch_eee(reduced_bulk=reduced_bulk,\\    # select a new batch using active learning\n",
    "                representatives=init_representatives,\\\n",
    "                cluster_labels=init_cluster_labels,\\\n",
    "                xdimsemis=xdimsemis,\\\n",
    "                xdimgts=xdimgts,\\\n",
    "                discount_rate = 1,\\\n",
    "                semi_dis_rate = 1,\\\n",
    "                batch_size=8\\\n",
    "               ):\n",
    "    # \n",
    "    lhet = []\n",
    "    lmp = [] \n",
    "    for i in range(len(representatives)):\n",
    "        cluster_heterogeneity,in_cluster_uncertainty,uncertain_patient=compute_cluster_heterogeneity(cluster_number=i,\\\n",
    "                            reduced_bulk=reduced_bulk,\\\n",
    "                           representatives=representatives,\\\n",
    "                            cluster_labels=cluster_labels,\\\n",
    "                            xdimsemis=xdimsemis,\\\n",
    "                            xdimgts=xdimgts,\\\n",
    "                            discount_rate = 1,\\\n",
    "                            semi_dis_rate = 1\\\n",
    "                           )\n",
    "        lhet.append(cluster_heterogeneity)\n",
    "        lmp.append(uncertain_patient)\n",
    "    \n",
    "    new_representatives = copy.deepcopy(representatives)\n",
    "    new_cluster_labels = copy.deepcopy(cluster_labels)\n",
    "    print('heterogeneities: ',lhet)\n",
    "    for i in range(batch_size):\n",
    "        new_num = len(new_representatives)\n",
    "        mp_index = np.array(lhet).argmax()\n",
    "        print(mp_index)\n",
    "        lhet[mp_index] = -999\n",
    "        bestp, new_cluster_labels, hets = best_patient(cluster_labels=new_cluster_labels,representatives=new_representatives,\\\n",
    "                 reduced_bulk=reduced_bulk,cluster_num=mp_index,new_num=new_num)\n",
    "        \n",
    "        new_representatives = new_representatives + [bestp]\n",
    "    \n",
    "    return new_representatives,new_cluster_labels\n",
    "\n",
    "def best_patient(cluster_labels=init_cluster_labels,representatives=init_representatives,\\\n",
    "                 reduced_bulk=reduced_bulk,cluster_num=0,new_num=None):\n",
    "    if new_num == None:\n",
    "        new_num = len(representatives)\n",
    "    pindices = np.where(np.array(cluster_labels)==cluster_num)[0]\n",
    "    representative = representatives[cluster_num]\n",
    "    hets=[]\n",
    "    potential_new_labels = []\n",
    "    for i in range(len(pindices)):\n",
    "        potential_new_label = copy.deepcopy(cluster_labels)\n",
    "        newrepre = pindices[i]\n",
    "        het = 0\n",
    "        if newrepre in representatives:\n",
    "            hets.append(9999)\n",
    "            potential_new_labels.append(potential_new_label)\n",
    "            continue\n",
    "        for j in range(len(pindices)):\n",
    "            brepre = reduced_bulk[representative]\n",
    "            brepre2 = reduced_bulk[newrepre]\n",
    "            bj = reduced_bulk[pindices[j]]\n",
    "            bdist1 = (brepre - bj)**2\n",
    "            bdist1 = bdist1.sum()\n",
    "            bdist1 = bdist1**0.5\n",
    "            bdist2 = (brepre2 - bj)**2\n",
    "            bdist2 = bdist2.sum()\n",
    "            bdist2 = bdist2**0.5\n",
    "            \n",
    "            if bdist1 > bdist2:\n",
    "                #print(pindices[j])\n",
    "                het = het + bdist2\n",
    "                potential_new_label[pindices[j]]=new_num\n",
    "            else:\n",
    "                het = het + bdist1\n",
    "        hets.append(het)\n",
    "        potential_new_labels.append(potential_new_label)\n",
    "    hets = np.array(hets)\n",
    "    bestp = pindices[np.argmin(hets)]\n",
    "    new_cluster_labels = potential_new_labels[np.argmin(hets)]\n",
    "    return bestp, new_cluster_labels, hets\n",
    "\n",
    "def update_membership(reduced_bulk=reduced_bulk,\\\n",
    "                      representatives=init_representatives,\\\n",
    "                      \n",
    "                     ):\n",
    "    new_cluster_labels = []\n",
    "    for i in range(len(reduced_bulk)):\n",
    "        \n",
    "        dists=[]\n",
    "        #dist to repres\n",
    "        for j in representatives:\n",
    "            bdist = (reduced_bulk[j] - reduced_bulk[i])**2 \n",
    "            bdist = bdist.sum()\n",
    "            bdist = bdist**0.5\n",
    "            dists.append(bdist)\n",
    "        membership = np.array(dists).argmin()\n",
    "        new_cluster_labels.append(membership)\n",
    "    return new_cluster_labels\n",
    "\n",
    "def compute_cluster_heterogeneity(cluster_number=0,\\\n",
    "                            reduced_bulk=reduced_bulk,\\\n",
    "                           representatives=init_representatives,\\\n",
    "                            cluster_labels=init_cluster_labels,\\\n",
    "                            xdimsemis=xdimsemis,\\\n",
    "                            xdimgts=xdimgts,\\\n",
    "                            discount_rate = 1,\\\n",
    "                            semi_dis_rate = 1\\\n",
    "                           ):\n",
    "    semiflag=0\n",
    "    \n",
    "    representative = representatives[cluster_number]\n",
    "    in_cluster_uncertainty = []\n",
    "    cluster_labels = np.array(cluster_labels)\n",
    "    cluster_patient_indices = np.where(cluster_labels==cluster_number)[0]\n",
    "    \n",
    "    for i in range(len(cluster_patient_indices)): # number of patients in this cluster except the representative\n",
    "        \n",
    "        patient_index = cluster_patient_indices[i]\n",
    "        \n",
    "        if patient_index in representatives:\n",
    "            in_cluster_uncertainty.append(0)\n",
    "            continue\n",
    "            \n",
    "        # distance between this patient and representative\n",
    "        bdist = (reduced_bulk[representative] - reduced_bulk[patient_index])**2 \n",
    "        bdist = bdist.sum()\n",
    "        bdist = bdist**0.5\n",
    "        \n",
    "        ma = np.array(xdimsemis[patient_index]).copy(order='C')\n",
    "        mb = np.array(xdimgts[representative]).copy(order='C')\n",
    "        sdist = (faiss_knn(ma,mb,n_neighbors=1).mean())\n",
    "        \n",
    "        semiloss = np.log(1+gts[patient_index].sum(axis=0))- np.log(1+semis[patient_index].sum(axis=0))\n",
    "        semiloss = semiloss**2\n",
    "        semiloss = semiloss.sum()\n",
    "        semiloss = semiloss**0.5\n",
    "        \n",
    "        #print(bdist,sdist,semiloss)\n",
    "        \n",
    "        uncertainty = bdist + sdist*discount_rate + semi_dis_rate * semiloss\n",
    "        \n",
    "        in_cluster_uncertainty.append(uncertainty)\n",
    "        \n",
    "    cluster_heterogeneity = np.array(in_cluster_uncertainty).sum()\n",
    "    \n",
    "    uncertain_patient = cluster_patient_indices[np.array(in_cluster_uncertainty).argmax()] \n",
    "\n",
    "    return cluster_heterogeneity,in_cluster_uncertainty,uncertain_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a62aff-4d06-4b1e-bed3-01f566d46a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e9fd66-5ffd-42e5-b332-64ed5a1a4784",
   "metadata": {},
   "source": [
    "### start semiprofiling loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de708c-90ae-473b-91f5-71571887d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi processes\n",
    "\n",
    "def multisemi(l,device,rnd):\n",
    "    if l ==[]:\n",
    "        return\n",
    "    for i in l:\n",
    "        print('start semiprofiling patient',str(i),'using',device)\n",
    "        os.system('/mnt/data/jingtao2/anaconda3/envs/deep/bin/python semicommand.py '+str(i)+' '+str(device) + ' '+str(rnd))\n",
    "    return \n",
    "\n",
    "def multirecon(i,device):\n",
    "    print('start recon patient',str(i),'using',device)\n",
    "    pid=pids[i]\n",
    "    fastrecon(pid=pid,tgtpid=None,device=device,k=15,diagw=1,vaesteps=100,gansteps=100,save=True,path=None)\n",
    "    return\n",
    "\n",
    "def multirecon2(i,reconmodel,device):\n",
    "    print('start recon2 patient',str(i),'using',device)\n",
    "    pid=pids[i]\n",
    "    reconst_pretrain2(pid,reconmodel,device,k=15,diagw=1.0,vaesteps=50,gansteps=50,save=True)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b267381-1959-4d8d-91c0-53bf1c86c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "histdic = np.load('training_hist/hist_BGCV01_CV0902.npy',allow_pickle=True)\n",
    "hists = histdic.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96068e7c-8c4d-4d15-88bb-141cafa11cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08b3f9-49ef-494d-9a65-8c151e2e6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(hists['bulk3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213d3d7-e355-4136-83a2-45f90cdb0438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa3742-9554-42dd-918f-d33de9e7fc2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    for seqp in range(40,len(sids),BATCH_SIZE):\n",
    "        ### load this rounds representatives and cluster labels\n",
    "        if seqp <= BATCH_SIZE:\n",
    "            rnd = 1\n",
    "            print('start initial round')\n",
    "            reprefile = 'training_rec/init_representatives_'+str(BATCH_SIZE)+'.txt' \n",
    "            clusterfile = 'training_rec/init_cluster_labels_'+str(BATCH_SIZE)+'.txt' \n",
    "\n",
    "            f = open(reprefile,'r')\n",
    "            lines=f.readlines()\n",
    "            representatives=[]\n",
    "            for l in lines:\n",
    "                representatives.append(int(l.strip().split()[0]))\n",
    "            f.close()\n",
    "\n",
    "            f= open(clusterfile,'r')\n",
    "            cluster_labels=[]\n",
    "            lines=f.readlines()\n",
    "            for l in lines:\n",
    "                cluster_labels.append(int(l.strip().split()[0]))\n",
    "            f.close()\n",
    "            \n",
    "        else:\n",
    "            rnd = int(seqp/4)\n",
    "            print('start round '+str(rnd))\n",
    "            reprefile = 'training_rec/eer_representatives_'+str(rnd)+'.txt' \n",
    "            clusterfile = 'training_rec/eer_cluster_labels_'+str(rnd)+'.txt' \n",
    "\n",
    "            f= open(reprefile,'r')\n",
    "            lines=f.readlines()\n",
    "            representatives=[]\n",
    "            for l in lines:\n",
    "                representatives.append(int(l.strip().split()[0]))\n",
    "            f.close()\n",
    "\n",
    "            f= open(clusterfile,'r')\n",
    "            cluster_labels=[]\n",
    "            lines=f.readlines()\n",
    "            for l in lines:\n",
    "                cluster_labels.append(int(l.strip().split()[0]))\n",
    "            f.close()\n",
    "        ### end of loading representatives and cluster labels\n",
    "\n",
    "\n",
    "\n",
    "        ### reconstruction stage 1\n",
    "        c=0\n",
    "        #procs=[]\n",
    "        for rp in representatives:#new_representatives:\n",
    "            pid = pids[rp]\n",
    "            if ('fast_reconst1_'+pid) in os.listdir('covid_models'):\n",
    "                print('recon' + str(pid) + 'exist')\n",
    "                continue\n",
    "            device = 'cuda:' + str(c%8)\n",
    "            fastrecon(pid=pid,tgtpid=None,device=device,k=15,diagw=1,vaesteps=100,gansteps=100,save=True,path=None)\n",
    "            \n",
    "            #proc = Process(target=multirecon, args=(rp,device))\n",
    "            #procs.append(proc)\n",
    "            #proc.start()\n",
    "            c=c+1\n",
    "        \n",
    "        #for proc in procs:\n",
    "        #        proc.join()\n",
    "        ### end of recon stage 1\n",
    "\n",
    "        ### reconstruction stage 2\n",
    "        i=0\n",
    "        c=0\n",
    "        procs=[]\n",
    "        for rp in representatives:##new_representatives:\n",
    "            pid = pids[rp]\n",
    "            device = 'cuda:' + str(c%8)\n",
    "            if ('fastreconst2_'+pid) in os.listdir('covid_models'):\n",
    "                print('recon2' + str(pid) + 'exist')\n",
    "                continue\n",
    "            adata,adj,variances,bulk,geneset_len = setdata(pid,None,device=device,k=15,diagw=1.0)\n",
    "            reconmodel = fastgenerator(adj = adj,variances = variances,markermask = None,bulk=bulk,geneset_len = geneset_len,adata=adata,\\\n",
    "                        n_hidden=256,n_latent=32,dropout_rate=0,countbulkweight=0,logbulkweight=0,absbulkweight=0,abslogbulkweight=0,\\\n",
    "                        power=2,corrbulkweight=0,meanbias=0)\n",
    "            reconmodel.module.load_state_dict(torch.load('covid_models/fast_reconst1_'+str(pid)))\n",
    "            reconst_pretrain2(pid,reconmodel,device,k=15,diagw=1.0,vaesteps=50,gansteps=50,save=True)\n",
    "            \n",
    "            \n",
    "            #proc = Process(target=multirecon2, args=(rp,reconmodel,device))\n",
    "            #procs.append(proc)\n",
    "            #proc.start()\n",
    "            #c=c+1\n",
    "            #i+=1\n",
    "        #for proc in procs:\n",
    "        #        proc.join()\n",
    "        ### end of recon stage 2\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        ### semi profiling\n",
    "        procs = []\n",
    "        c=0\n",
    "\n",
    "        l0 = []\n",
    "        l1 = []\n",
    "        l2 = []\n",
    "        l3 = []\n",
    "        l4 = []\n",
    "        l5 = []\n",
    "        l6 = []\n",
    "        l7 = []\n",
    "\n",
    "        for i in range(0,30):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l0.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(30,60):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l1.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(60,90):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l2.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(90,len(sids)):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l3.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "                \n",
    "        '''\n",
    "        for i in range(56,70):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l4.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(70,84):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l5.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(84,98):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l6.append(i)\n",
    "            else:\n",
    "                print('exist')\n",
    "        for i in range(98,112):\n",
    "            if i in representatives:\n",
    "                continue\n",
    "            reprepid = pids[representatives[cluster_labels[i]]]\n",
    "            if ('fast'+ reprepid+'_to_'+str(pids[i])+'.npy') not in os.listdir('semidata'):\n",
    "                l7.append(i)\n",
    "            else:\n",
    "                print('exist')'''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        proc = Process(target=multisemi, args=(l0,'cuda:0',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l1,'cuda:1',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l2,'cuda:2',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l3,'cuda:3',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "    \n",
    "        '''\n",
    "        proc = Process(target=multisemi, args=(l4,'cuda:4',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l5,'cuda:5',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l6,'cuda:6',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "        proc = Process(target=multisemi, args=(l7,'cuda:7',rnd))\n",
    "        procs.append(proc)\n",
    "        proc.start()'''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "        ### end semi \n",
    "\n",
    "\n",
    "        ### load new semi results\n",
    "        t_start = timeit.default_timer()\n",
    "        semis=[]\n",
    "        for i in range(len(pids)):\n",
    "            pid = pids[i]\n",
    "            repre = representatives[cluster_labels[i]]\n",
    "            reprepid = pids[repre]\n",
    "            if reprepid==pid:\n",
    "                semis.append(gts[i])\n",
    "            else:\n",
    "                xsem = np.load('semidata/fast'+ reprepid+'_to_'+pid+'.npy')\n",
    "                xsem = xsem*(xsem>10)                                   ############### thresholding\n",
    "                semis.append(xsem)\n",
    "            print(i,end=', ')\n",
    "\n",
    "        t_end = timeit.default_timer()\n",
    "        print()\n",
    "        print(str(t_end-t_start),'for loading semi')\n",
    "        ### end of loading semi\n",
    "\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "        ### PCA\n",
    "        t_start = timeit.default_timer()\n",
    "        \n",
    "        X = np.concatenate([np.concatenate(gts,axis=0),np.concatenate(semis,axis=0)],axis=0)\n",
    "        X = np.log(X+1)\n",
    "        reducer =  PCA(n_components = 100)#PCA(n_components = 100)#,svd_solver = 'randomized')#randomized_svd(n_components=100)  #PCA(n_components=100)#\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "\n",
    "        t_end = timeit.default_timer()\n",
    "        print()\n",
    "        print(str(t_end-t_start),'for pca')\n",
    "        ### end of pca\n",
    "\n",
    "\n",
    "        ### reduced data for patients\n",
    "        xdimgts=[]\n",
    "        xdimsemis=[]\n",
    "        offset=0\n",
    "        xused = X_reduced#X_UMAP # X_PCA\n",
    "        for i in range(len(pids)):\n",
    "            xdimgts.append(xused[offset:(offset+gts[i].shape[0]),:])\n",
    "            offset = offset+gts[i].shape[0]\n",
    "        lengt = offset\n",
    "        for i in range(len(pids)):\n",
    "            xdimsemis.append(xused[offset:(offset+semis[i].shape[0]),:])\n",
    "            offset = offset+semis[i].shape[0]\n",
    "        ### end \n",
    "\n",
    "        \n",
    "        ### lowerbound\n",
    "        lbscores2 = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        lbgt = copy.deepcopy(X_reduced)\n",
    "        np.random.shuffle(lbgt)\n",
    "        lbgt1 = lbgt[:lbgt.shape[0]//2,:]\n",
    "        lbgt2 = lbgt[lbgt.shape[0]//2:,:]\n",
    "        \n",
    "        #lbgt1 = copy.deepcopy(X_reduced)\n",
    "        #lbgt2 = copy.deepcopy(X_reduced)\n",
    "        ma = np.array(lbgt1).copy(order='C')\n",
    "        mb = np.array(lbgt2).copy(order='C')\n",
    "        lowerbound = list(faiss_knn(ma,mb,n_neighbors=1)) + list(faiss_knn(mb,ma,n_neighbors=1))\n",
    "        lowerbound = np.array(lowerbound)\n",
    "        lowerbound = lowerbound.mean()\n",
    "\n",
    "        #lowerbound = 40\n",
    "        \n",
    "        ### upperbound\n",
    "        ubscores = []\n",
    "        for i in range(len(pids)):\n",
    "            gt = xdimgts[i]\n",
    "            randomidx = np.random.randint(0,len(pids))\n",
    "            gtr = xdimgts[randomidx]\n",
    "            ma = np.array(gt).copy(order='C')\n",
    "            mb = np.array(gtr).copy(order='C')\n",
    "            ubscore = list(faiss_knn(ma,mb,n_neighbors=1)) + list(faiss_knn(mb,ma,n_neighbors=1))\n",
    "            ubscores.append(np.array(ubscore).mean())\n",
    "        upperbound = np.array(ubscores).mean()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### semi evaluation\n",
    "        t_start = timeit.default_timer()\n",
    "        scores = []\n",
    "        for i in range(len(pids)):\n",
    "            pid = pids[i]\n",
    "            if i in representatives:\n",
    "                scores.append(lowerbound)\n",
    "                continue\n",
    "            gt = xdimgts[i]\n",
    "            xs = xdimsemis[i]\n",
    "            ma = np.array(gt).copy(order='C')\n",
    "            mb = np.array(xs).copy(order='C')\n",
    "            err1 = faiss_knn(ma,mb,n_neighbors=1)\n",
    "            err2 = faiss_knn(mb,ma,n_neighbors=1)\n",
    "            err = list(err1) + list(err2)\n",
    "            err = (np.array(err)).mean()\n",
    "            scores.append(err)\n",
    "        semierror = np.array(scores).mean()\n",
    "\n",
    "        ### naive evaluation\n",
    "        naivescores = []\n",
    "        for i in range(len(pids)):\n",
    "            pid = pids[i]\n",
    "            if i in representatives:\n",
    "                naivescores.append(lowerbound)\n",
    "                continue\n",
    "            gt = xdimgts[i]\n",
    "            repre = representatives[cluster_labels[i]]\n",
    "            xs = xdimgts[repre]\n",
    "            ma = np.array(gt).copy(order='C')\n",
    "            mb = np.array(xs).copy(order='C')\n",
    "            err1 = faiss_knn(ma,mb,n_neighbors=1)\n",
    "            err2 = faiss_knn(mb,ma,n_neighbors=1)\n",
    "            err = list(err1) + list(err2)\n",
    "            err = (np.array(err)).mean()\n",
    "            naivescores.append(err)\n",
    "        naiveerror = np.array(naivescores).mean()\n",
    "\n",
    "\n",
    "        normednaive = (naiveerror - lowerbound)/(upperbound - lowerbound)\n",
    "        normedsemi = (semierror - lowerbound)/(upperbound - lowerbound)\n",
    "        t_end = timeit.default_timer()\n",
    "        print('naive:',naiveerror)\n",
    "        print('semi:',semierror)\n",
    "        print('upperbound:',upperbound)\n",
    "        print('lowerbound;',lowerbound)\n",
    "        print('normed naive:',normednaive)\n",
    "        print('normed semi:',normedsemi)\n",
    "        print(str(t_end-t_start),' for evaluation')\n",
    "        \n",
    "        f=open('performance/semi_pca_t10_'+str(rnd)+'.txt','w')\n",
    "        f.write('naive:'+str(naiveerror)+'\\n')\n",
    "        f.write('semi:'+str(semierror)+'\\n')\n",
    "        f.write('upperbound:'+str(upperbound)+'\\n')\n",
    "        f.write('lowerbound;'+str(lowerbound)+'\\n')\n",
    "        f.write('normed naive:'+str(normednaive)+'\\n')\n",
    "        f.write('normed semi:'+str(normedsemi)+'\\n')\n",
    "        f.close()\n",
    "        ### end of evaluation\n",
    "\n",
    "\n",
    "        ### active learning \n",
    "        nrep, nlabels = pick_batch_eee(reduced_bulk=reduced_bulk,\\\n",
    "                        representatives=representatives,\\\n",
    "                        cluster_labels=cluster_labels,\\\n",
    "                        xdimsemis=xdimsemis,\\\n",
    "                        xdimgts=xdimgts,\\\n",
    "                        discount_rate = 1,\\\n",
    "                        semi_dis_rate = 1,\\\n",
    "                        batch_size=4\\\n",
    "                       )\n",
    "        new_representatives = nrep\n",
    "        new_cluster_labels = nlabels\n",
    "        f=open('training_rec/eer_cluster_labels_'+str(rnd+1)+'.txt','w')\n",
    "        for i in range(len(new_cluster_labels)):\n",
    "            f.write(str(new_cluster_labels[i])+'\\n')\n",
    "        f.close()\n",
    "        f=open('training_rec/eer_representatives_'+str(rnd+1)+'.txt','w')\n",
    "        for i in range(len(new_representatives)):\n",
    "            f.write(str(new_representatives[i])+'\\n')\n",
    "        f.close()\n",
    "\n",
    "        cluster_labels = new_cluster_labels\n",
    "        representatives = new_representatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393e0c2-4c9d-4d93-ac33-a8677ba550f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ae5ad-07e0-43a0-afce-9d1d25f84d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1786c-f883-48e5-976d-b4868c764543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
